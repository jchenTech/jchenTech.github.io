<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>深度学习 on 陈建君的技术博客</title>
    <link>http://jchenTech.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 深度学习 on 陈建君的技术博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 03 Apr 2020 17:57:06 +0800</lastBuildDate>
    
	<atom:link href="http://jchenTech.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>循环神经网络RNN介绍</title>
      <link>http://jchenTech.github.io/2020/04/03/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Fri, 03 Apr 2020 17:57:06 +0800</pubDate>
      
      <guid>http://jchenTech.github.io/2020/04/03/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn%E4%BB%8B%E7%BB%8D/</guid>
      <description>循环神经网络(Recurrent Neural Networks，RNN)已经在众多自然语言处理(Natural Language Processing, NLP)中取得了巨大成功以及广泛应用。本文将对RNN进行简单介绍。
1. 神经网络基础 神经网络可以当做是能够拟合任意函数的黑盒子，只要训练数据足够，给定特定的x，就能得到希望的y，结构图如下：
将神经网络模型训练好之后，在输入层给定一个x，通过网络之后就能够在输出层得到特定的y，那么既然有了这么强大的模型，为什么还需要RNN（循环神经网络）呢？
2. 为什么需要RNN 他们都只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。
 比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列； 当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。
 以nlp的一个最简单词性标注任务来说，将我 吃 苹果 三个单词标注词性为 我/nn 吃/v 苹果/nn。
那么这个任务的输入就是：
我 吃 苹果 （已经分词好的句子）
这个任务的输出是：
我/nn 吃/v 苹果/nn(词性标注好的句子)
对于这个任务来说，我们当然可以直接用普通的神经网络来做，给网络的训练数据格式了就是我-&amp;gt; 我/nn 这样的多个单独的单词-&amp;gt;词性标注好的单词。
但是很明显，一个句子中，前一个单词其实对于当前单词的词性预测是有很大影响的，比如预测苹果的时候，由于前面的吃是一个动词，那么很显然苹果作为名词的概率就会远大于动词的概率，因为动词后面接名词很常见，而动词后面接动词很少见。
所以为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就诞生了。
3. RNN结构 首先看一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：
不知道初学的同学能够理解这个图吗，反正我刚开始学习的时候是懵逼的，每个结点到底代表的是一个值的输入，还是说一层的向量结点集合，如何隐藏层又可以连接到自己，等等这些疑惑~这个图是一个比较抽象的图。
我们现在这样来理解，如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的全连接神经网络。x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；
U是输入层到隐藏层的权重矩阵，o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。
那么，现在我们来看看W是什么。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。
我们给出这个抽象图对应的具体图：
我们从上图就能够很清楚的看到，上一时刻的隐藏层是如何影响当前时刻的隐藏层的。
如果我们把上面的图展开，循环神经网络也可以画成下面这个样子：
我们可以用下面的公式来表示循环神经网络的计算方法：
4. 总结 好了，到这里大概讲解了RNN最基本的几个知识点，能够帮助大家直观的感受RNN和了解为什么需要RNN，后续总结它的反向求导知识点。
注意：为了简单说明问题，偏置都没有包含在公式里面。</description>
    </item>
    
    <item>
      <title>深度学习目标检测：R-CNN, SSPNet, Fast R-CNN, Faster R-CNN, YOLO, SSD</title>
      <link>http://jchenTech.github.io/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Br-cnn-sspnet-fast-r-cnn-faster-r-cnn-yolo-ssd/</link>
      <pubDate>Fri, 03 Apr 2020 16:28:06 +0800</pubDate>
      
      <guid>http://jchenTech.github.io/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Br-cnn-sspnet-fast-r-cnn-faster-r-cnn-yolo-ssd/</guid>
      <description>&lt;p&gt;Object Detection，在给定的图像中，找到目标图像的位置，并标注出来。 或者是，图像中有那些目标，目标的位置在那。这个目标，是限定在数据集中包含的目标种类，比如数据集中有两种目标:狗，猫。 就在图像找出来猫，狗的位置，并标注出来 是狗还是猫。&lt;/p&gt;

&lt;p&gt;这就涉及到两个问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;目标识别，识别出来目标是猫还是狗，Image Classification解决了图像的识别问题。&lt;/li&gt;
&lt;li&gt;定位，找出来猫狗的位置。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文将详细介绍目前深度学习目标检测算法的几种热门算法R-CNN, SSPNet, Fast R-CNN, Faster R-CNN, YOLO, SSD的比较&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>卷积神经网络</title>
      <link>http://jchenTech.github.io/2020/03/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Sun, 29 Mar 2020 13:58:06 +0800</pubDate>
      
      <guid>http://jchenTech.github.io/2020/03/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
      <description>&lt;p&gt;卷积神经网络（CNN）是一种深层神经网络，已被证明在计算机视觉任务中表现出色，例如图像分类，目标检测，目标定位和神经样式转换。 在这篇文章中，我将详细地介绍构成卷积神经网络的不同层：卷积层，池化层和完全连接层。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jchenTech.github.io/images/卷积神经网络/卷积神经网络实例_汽车图像识别.jpg&#34; alt=&#34;卷积神经网络实例-汽车图像识别&#34; /&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>