<!DOCTYPE HTML>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /> 
    <title>【论文笔记】AOD-Net: An All-in-One Network for Dehazing and Beyond - 陈建君的技术博客</title>
    <meta name="keywords" content="蘭陵N散記,兰陵,独立,博客,程序员,架构师,个人,思考,读书,笔记,技术,分享,Golang">
    
    <meta property="og:title" content="【论文笔记】AOD-Net: An All-in-One Network for Dehazing and Beyond">
    <meta property="og:site_name" content="陈建君的技术博客">
    <meta property="og:image" content="/img/author.jpg"> 
    <meta name="title" content="【论文笔记】AOD-Net: An All-in-One Network for Dehazing and Beyond - 陈建君的技术博客" />
    <meta name="description" content="蘭陵N散記 | 博客 | 软件 | 架构 | Java | Golang"> 
    <link rel="shortcut icon" href="http://jchenTech.github.io/img/favicon.ico" />
    <link rel="apple-touch-icon" href="http://jchenTech.github.io/img/apple-touch-icon.png" />
    <link rel="apple-touch-icon-precomposed" href="http://jchenTech.github.io/img/apple-touch-icon.png" />
    <link href="http://jchenTech.github.io/js/vendor/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />
    <link href="http://jchenTech.github.io/js/vendor/fancybox/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />
    <link href="http://jchenTech.github.io/css/main.css" rel="stylesheet" type="text/css" />
    <link href="http://jchenTech.github.io/css/syntax.css" rel="stylesheet" type="text/css" />
    <script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
     fancybox: true, 
    motion: true
  };
</script>
</head>
<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">
<div class="container one-collumn sidebar-position-left page-home  ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-meta  custom-logo ">

  <div class="custom-logo-site-title">
    <a href="http://jchenTech.github.io/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">陈建君的技术博客</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">一指流沙，程序年华</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
    <ul id="menu" class="menu">
      
      
        <li class="menu-item ">
          <a href="http://jchenTech.github.io/" rel="section">
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页
          </a>
        </li>
      
        <li class="menu-item ">
          <a href="http://jchenTech.github.io/categories/%E6%8A%80%E6%9C%AF/" rel="section">
              <i class="menu-item-icon fa fa-fw fa-code"></i> <br />技术
          </a>
        </li>
      
        <li class="menu-item ">
          <a href="http://jchenTech.github.io/categories/%E6%84%9F%E6%83%B3/" rel="section">
              <i class="menu-item-icon fa fa-fw fa-tint"></i> <br />感想
          </a>
        </li>
      
        <li class="menu-item ">
          <a href="http://jchenTech.github.io/categories/%E7%AC%94%E8%AE%B0/" rel="section">
              <i class="menu-item-icon fa fa-fw fa-book"></i> <br />笔记
          </a>
        </li>
      
        <li class="menu-item ">
          <a href="http://jchenTech.github.io/categories/%E6%9D%82%E8%AE%B0/" rel="section">
              <i class="menu-item-icon fa fa-fw fa-leaf"></i> <br />杂记
          </a>
        </li>
      
        <li class="menu-item ">
          <a href="http://jchenTech.github.io/post/" rel="section">
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档
          </a>
        </li>
      
        <li class="menu-item ">
          <a href="http://jchenTech.github.io/about/" rel="section">
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于
          </a>
        </li>
      
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger"> <i class="menu-item-icon fa fa-search fa-fw"></i> <br /> 搜索</a>
      </li>
    </ul>
    <div class="site-search">
      <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>

    </div>
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
<section id="posts" class="posts-expand">
  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
      <h1 class="post-title" itemprop="name headline">
        <a class="post-title-link" href="http://jchenTech.github.io/post/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0aod-net/" itemprop="url">
        【论文笔记】AOD-Net: An All-in-One Network for Dehazing and Beyond
        </a>
      </h1>
      <div class="post-meta">
      <span class="post-time">
<span class="post-meta-item-icon">
    <i class="fa fa-calendar-o"></i>
</span>
<span class="post-meta-item-text">时间：</span>
<time itemprop="dateCreated" datetime="2016-03-22T13:04:35+08:00" content="2020-07-18">
    2020-07-18
</time>
</span> 
      

  <span class="post-category" >
  &nbsp; | &nbsp;
  <span class="post-meta-item-icon">
    <i class="fa fa-folder-o"></i>
  </span>
  <span class="post-meta-item-text">分类：</span>
  
    <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
      <a href="http://jchenTech.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" itemprop="url" rel="index">
        <span itemprop="name">图像处理与深度学习</span>
      </a>
      &nbsp; 
    </span>
  
</span>


       <span>
&nbsp; | &nbsp;
<span class="post-meta-item-icon">
    <i class="fa fa-eye"></i>
</span>
<span class="post-meta-item-text">阅读：</span>
<span class="leancloud-visitors-count">10253 字 ~21分钟</span>
</span>
      </div>
    </header>
    <div class="post-body" itemprop="articleBody">
    <blockquote>
<p>摘要:本文提出了一种用卷积神经网络（CNN）构建的图像去雾模型，称为一体化除雾网络（AOD-Net）。它是基于重新配制的大气散射模型设计的。 AOD-Net不是像大多数先前模型那样分别估算传输矩阵和大气光，而是通过轻量级CNN直接生成清晰图像。这种新颖的端到端设计使得将AOD-Net嵌入到其他深度模型（例如，faster R-CNN）中变得容易，以改善对模糊图像的高级任务性能。在合成和自然模糊图像数据集上的实验结果证明了我们在PSNR，SSIM和主观视觉质量方面优于现有技术的优越性能。此外，当将AOD-Net与faster R-CNN连接并从头到尾训练联合管道时，我们目睹了对模糊图像的对象检测性能的大幅提升。</p>

<p>关键词:去雾，图像恢复，深度学习，联合训练，物体检测。</p>
</blockquote>

<p><img src="http://jchenTech.github.io/images/AOD-Net/网络框架图.png" alt="网络框架图" /></p>

<p>论文下载：<a href="https://arxiv.org/pdf/1707.06543.pdf">https://arxiv.org/pdf/1707.06543.pdf</a></p>

<p>tensorflow源码 ：<a href="https://download.csdn.net/download/cherry1307/11012233">https://download.csdn.net/download/cherry1307/11012233</a></p>

<p>pythorch源码: <a href="https://github.com/TheFairBear/PyTorch-Image-Dehazing">https://github.com/TheFairBear/PyTorch-Image-Dehazing</a></p>

<p>项目：<a href="https://sites.google.com/site/boyilics/website-builder/project-page">https://sites.google.com/site/boyilics/website-builder/project-page</a></p>

<h2 id="引言">引言</h2>

<p>由于存在诸如灰尘，雾气和烟雾的气溶胶，雾度的存在给相机捕获的图像增加了复杂的噪声。 它会大大降低室外图像的可见度，对比度降低，表面颜色变暗。 此外，模糊的图像将使许多后续高级计算机视觉任务的有效性受到威胁，例如物体检测和识别。 因此，广泛考虑了去雾算法，作为（不适定的）图像恢复和增强的具有挑战性的实例。 类似于图像去噪和超分辨率[37]，[15]等其他问题，早期的去雾工作[23]，[30]，[38]，[12]假设来自同一场景的多个图像的可用性。 然而，从单个图像中去除雾霾现在已经占据了主导地位，因为它对于实际设置更为实用[7]。 本文重点研究了单幅图像去雾问题。</p>

<h3 id="前人的工作">前人的工作</h3>

<p>作为用于去雾的先验知识，模糊图像生成遵循广为接受的物理模型（详见第II-A节）。除了估计全球大气光强度之外，已经认识到实现雾霾去除的关键是传输矩阵的恢复。</p>

<p>[7]通过估计场景的反照率提出了一种物理接地的方法。</p>

<p>[9]，[34]发现有效暗通道先验（DCP）更可靠地计算传输矩阵，然后是一系列工作</p>

<p>[13]，[24]，[36]。 [20]强制执行边界约束和上下文正则化以获得更清晰的恢复图像。在</p>

<p>[33]中提出了一种自动恢复大气光的加速方法。</p>

<p>[45]先前开发了颜色衰减，并为模糊图像创建了场景深度的线性模型，然后以监督的方式学习了模型参数。</p>

<p>[16]说明了联合估计场景深度并从模糊视频序列中恢复清晰潜像的方法。</p>

<p>[1]提出了一种基于非局部先验（雾度线）的算法，基于清晰图像中的每个颜色簇成为RGB空间中的雾度线的假设。</p>

<p>所有上述方法都取决于物理模型和各种复杂的图像统计假设。然而，由于从单个图像估计物理参数通常是不准确的，因此上述方法的去雾性能似乎并不总是令人满意。</p>

<p>最近，随着卷积神经网络（CNN）在计算机视觉任务中取得了普遍的成功，它们也被引入到图像去雾中。</p>

<p>DehazeNet [3]提出了一种可训练的模型来估计一个模糊图像的传输矩阵。</p>

<p>[27]进一步开发了多尺度CNN（MSCNN），它首先生成了一个粗尺度的传输矩阵，然后对其进行了细化。</p>

<h3 id="存在的问题">存在的问题</h3>

<p>1）没有端到端去雾：大多数用于图像恢复和增强的深度学习方法完全包含端到端建模：训练模型以直接从损坏的图像中回归清晰的图像。例子包括图像去噪[42]，去模糊[31]和超分辨率[41]。相比之下，迄今为止还没有用于除雾的端到端深度模型，它直接从模糊的模型中回归清晰的图像。虽然乍一看可能看起来很奇怪，但人们需要意识到雾霾本质上会带来不均匀的依赖于信号的噪声：由雾度引起的表面场景衰减与相机表面之间的物理距离相关（即像素深度）。这与假设与信号无关的噪声的大多数图像劣化模型不同，在这种情况下，所有信号都经历相同的参数化降级过程。因此，可以使用一个静态映射功能轻松建模其恢复模型。同样不能直接应用于除雾：降解过程因信号而异，并且恢复模型也必须是输入自适应的。</p>

<p>现有方法具有相同的信念，即估计精确的中间传输图是从雾霾中恢复清晰的场景的关键[1]，[3]，[27]。通过经验规则分别计算大气光，并且基于物理模型恢复清晰图像。尽管是直观的，但是这样的过程不直接测量或最小化重建失真。用于估计传输矩阵和大气光的两个单独步骤中的误差将累积并且可能相互放大。结果，传统的单独管道产生了次优的图像恢复质量。</p>

<p>2）缺少与高级视觉任务的衔接：目前，去雾模型依赖于两组评估标准：（1）对于合成模糊图像，其对应的清晰图像已知，通常计算PSNR和SSIM来测量确保恢复保真度; （2）对于没有清晰图像的真实自然模糊图像，唯一可用的去雾效果比较是主观视觉质量。然而，与图像去噪和超分辨率结果不同，他们视觉伪像的抑制效果是可见的（例如，在纹理和边缘上），现有技术的去雾模型之间的视觉差异[1]，[3]，[27]通常表现在全球的光照和色调中，并且通常太微妙而无法分辨。</p>

<p>一般图像恢复和增强，被称为低级视觉任务的一部分，通常被认为是中级和高级视觉任务的预处理步骤。众所周知，诸如物体检测和识别之类的高级计算机视觉任务的性能将在存在各种劣化的情况下恶化，并且然后在很大程度上受到图像恢复和增强的质量的影响。然而，根据我们的最佳知识，没有探索将去雾算法和结果与高级视觉任务性能相关联。</p>

<h3 id="我们的贡献">我们的贡献</h3>

<p>在本文中，我们提出了一体化除雾网络（AOD-Net），这是一种基于CNN的除雾模型，具有两个关键创新，以应对上述两个挑战：</p>

<ul>
<li><p>我们是第一个提出端到端可训练的去雾模型，它直接从模糊图像中生成干净的图像，而不是依赖于任何单独的和中间的参数估计步骤1。 。 AOD-Net是基于重新配制的大气散射模型设计的，因此保留了与现有工作相同的物理基础[3]，[27]。然而，它建立在我们不同的信念之上，即物理模型可以以“更端对端”的方式制定，其所有参数在一个统一模型中估算。</p></li>

<li><p>我们是第一个定量研究去雾质量如何影响随后的高级视力任务的人，这项任务是比较去雾效果的新客观标准。此外，AOD-Net可以与其他深层模型无缝嵌入，构成一个管道，在模糊图像上执行高级任务，具有隐式的去雾过程。由于我们独特的一体化设计，这种管道可以从头到尾联合调整，以进一步提高性能，如果用其他深度去雾方法替代AOD-Net是不可行的[3]，[27]。</p></li>
</ul>

<p>AOD-Net在合成模糊图像上进行训练，并在合成和真实自然图像上进行测试。实验证明了AOD-Net优于几种最先进的方法，不仅包括PSNR和SSIM（见图1），还包括视觉质量（见图2）。作为轻量级和高效率的型号，AOD-Net的耗费时间低至0.026秒，可以使用单个GPU处理一个480×640图像。当与更快的R-CNN [26]连接时，AOD-Net在改善模糊图像的物体检测性能方面明显优于其他去雾模型，当我们共同调整AOD-Net和fasterrcnn的流水线时，性能得到进一步提升。</p>

<p>本文从以前的会议版本[14]扩展而来。本文最引人注目的改进在于第四部分，我们对评估和增强物体检测的去雾进行了深入的讨论，并介绍了联合训练部分的细节和分析。我们还对AOD-Net的架构进行了更详细和全面的分析（例如第III-D节）。此外，我们还包括更广泛的比较结果。</p>

<h2 id="提出的方法">提出的方法</h2>

<h3 id="物理模型">物理模型</h3>

<p>在本节中，解释了提出的AOD-Net。 我们首先介绍了转换后的大气散射模型，在此基础上设计了AOD-Net。 然后详细描述AOD-Net的体系结构。</p>

<p>A 物理模型和转换公式</p>

<p>大气光散射模型已经是雾霾图像生成的经典描述模型[19], [21], [22]，</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/公式1.png" alt="" /></p>

<p>其中I(x)是雾霾图像，J(x)是场景辐射（即待恢复的清晰图像），这里有两个关键的参数：A代表大气光值，t(x)代表传输矩阵：</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/公式2.png" alt="" /></p>

<p>这里beta是大气光的散射系数，d(x)是物体和相机之间的距离，我们可以为清晰图像重写 模型：</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/公式3.png" alt="" /></p>

<p>现存的诸如[27]，[3]之类的工作遵循相同的三步程序：</p>

<p>1）使用复杂的深模型从模糊图像I（x）估计传输矩阵t（x）;</p>

<p>2）使用一些经验方法估计A;</p>

<p>3）通过（3）估计清洁图像J（x）。</p>

<p>这样的过程导致次优解决方案，其不直接最小化图像重建误差。当将它们组合在一起以计算（3）时，t（x）和A的单独估计将导致累积或甚至放大的误差。</p>

<p><strong>我们的核心思想是将两个参数t（x）和A统一为一个公式，即（4）中的K（x），并直接最小化像素域重建误差。为此，将（3）中的公式重新表示为下面的转换公式：</strong></p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/公式4.png" alt="" /></p>

<h3 id="网络设计">网络设计</h3>

<p><img src="http://jchenTech.github.io/images/AOD-Net/网络框架图.png" alt="网络框架图" /></p>

<p>提出的aod-net包含两个模块：</p>

<p>（a）k-estimation模块：从输入图像I(x)估计k（x）;</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/k-estimation模块.png" alt="k-estimation模块" /></p>

<p>（b）清晰图像生成模型：k（x）再去估计J（x）</p>

<p><strong>K估算模块是AOD-Net的关键组成部分，负责估算深度和雾浓度</strong>。如图4（b）所示，我们使用五个卷积层，并通过<strong>融合不同大小的卷积核形成多尺度特征</strong>。在[3]中，在第二层中使用具有不同卷积核尺寸的并行卷积。[27]将粗尺度网络特征与细尺度网络的中间层连接起来。受他们的启发，AOD-Net的“concat1”层连接层“conv1”和“conv2”的特征。类似的，“concat2”连接来自“conv2”和“conv3”的特征，“concat3”连接来自“conv3”和“conv4”的特征。这种多尺度设计捕获不同尺度的特征，并且中间连接还不长卷积期间的信息损失。值得注意的是，<strong>AOD-Net的每个卷积层仅使用三个卷积核</strong>。因此与现有的深层方法相比，AOD-Net重量轻，例如[3],[27]。在K估计模块之后，清晰图像生成块由逐元素乘法层和若干元素加法层组成，以便通过计算（4）生成恢复的图像。</p>

<h2 id="实验结果">实验结果</h2>

<h3 id="数据集和实现">数据集和实现</h3>

<p>创建合成数据集：NYU2深度数据集，大气光值[0.6,1.0],beta {0.4,0.6,0.8,1.0,1.2,1.4,1.6},取出27256个图像做训练集，3170个做测试集A，Middlebury立体数据集中取出800张full-size合成图像作为测试集B,此外，在自然雾霾图像上也评估了模型的泛化能力。</p>

<p>在训练过程中，使用高斯随机变量初始化权重。 我们使用ReLU神经元，因为我们发现它比[3]中提出的BReLU神经元在我们的特定环境中更有效。 动量和衰减参数分别设置为0.9和0.0001。 我们使用8个图像（480×640）的批量大小，学习率为0.001。 我们采用简单的均方误差（MSE）损失函数，并且很高兴地发现它不仅提升了PSNR，还提升了SSIM以及视觉质量。AOD-Net模型需要大约10个训练时期才能收敛，并且通常在10个时期之后表现得足够好。 在本文中，我们已经训练了40个时期的模型。 还发现将范数约束在[-0.1,0.1]内对clip the gradient （防止梯度爆炸）很有帮助。 该技术在稳定复发网络训练方面很受欢迎[25]。</p>

<h3 id="合成数据集上的质量评估">合成数据集上的质量评估</h3>

<p>对比方法：</p>

<ol>
<li>Fast Visibility Restoration (FVR) [35],</li>
<li>Dark-Channel Prior (DCP) [9],</li>
<li>Boundary Constrained Context Regularization (BCCR) [20],</li>
<li>Automatic Atmospheric Light Recovery (ATM) [33],</li>
<li>Color Attenuation Prior (CAP) [45],</li>
<li>Non-local Image Dehazing (NLD) [1], [2],</li>
<li>DehazeNet [3], and</li>
<li>MSCNN [27]</li>
</ol>

<p>在之前的实验中，由于在真实模糊图像上进行测试时没有无雾霾的真实性，因此报告了很少有关于恢复质量的定量结果。 我们合成的模糊图像伴随着地面实况图像，使我们能够根据PSNR和SSIM比较这些去噪结果。</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/Table1.png" alt="" /></p>

<p>表I和III-B分别显示了测试集A和B上的平均PSNR和SSIM结果。 由于AOD-Net在MSE损失下从端到端进行了优化，因此看到其PSNR性能高于其他方法并不令人惊讶。 更有吸引力的是观察到AOD-Net比所有竞争对手获得更大的SSIM优势，尽管SSIM并未直接被称为优化标准。 由于SSIM的测量超出了像素方面的误差，并且众所周知，它更忠实地反映了人类的感知，因此我们对AOD-Net的哪一部分实现了这种持续改进感到好奇。</p>

<p>我们进行以下调查：TestSet B中的每个图像被分解为平均图像和残差图像的总和。前者由具有相同平均值的所有像素位置构成（图像上的平均3通道矢量）。很容易证明两个图像之间的MSE等于它们在两个残留图像之间添加的平均图像之间的MSE。平均图像大致对应于全局照明并且与A相关，而残差更多地涉及局部结构变化和对比等。我们观察到AOD-Net产生类似的残余MSE（在TestSet B上平均）到一些竞争对手DehazeNet和CAP等方法。然而，AOD-Net结果的平均部分的MSE显着低于DehazeNet和CAP，如表III所示。由此暗示，由于我们在端到端重建损失下的联合参数估计方案，AOD-Net可能更能够正确地恢复A（全局照明）。由于人眼对全局照明的大变化肯定比对任何局部失真更敏感，因此难怪为什么AOD-Net的视觉效果也明显更好，而其他一些结果通常看起来不切实际。</p>

<p>上述优点也体现在计算SSIM [39]的照明（1）项中，并部分解释了我们强大的SSIM结果。 SSIM收益的另一个主要来源似乎来自对比（c）项。例如，我们随机选择五个图像进行测试，TestSetB上AOD-Net结果的对比值平均值为0.9989，显着高于ATM（0.7281），BCCR（0.9574），FVR（0.9630），NLD（0.9250），DCP （0.9457），MSCNN（0.9697），DehazeNet（0.9076）和CAP（0.9760）。</p>

<h3 id="视觉效果评估">视觉效果评估</h3>

<p>a）合成图像：图5显示了来自TestSet A的合成图像的去雾结果。我们观察到AOD-Net结果通常具有更清晰的轮廓和更丰富的颜色，并且在视觉上更忠实于地面实况。</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/Fig5.png" alt="" /></p>

<p>b）具有挑战性的自然图像：虽然通过室内图像合成训练，但ADO-Net可以很好地泛化为室外图像。我们根据一些自然图像示例的最新方法对其进行评估，这些示例比[9]，[8]，[3]的作者发现的一般室外图像更具挑战性。挑战在于高度杂乱的物体，精细纹理或照明变化的主导地位。如图6所示，FVR遭受过度增强的视觉伪影。 DCP，BCCR，ATM，NLD和MSCNN在一个或多个图像上产生不切实际的色调，例如第二行上的DCP，BCCR和ATM结果（注意天空颜色），或第四行上的BCCR，NLD和MSCNN结果（注意石头颜色）。 CAP，DehazeNet和AOD-Net拥有最具竞争力的视觉效果，具有合理的细节。然而，仔细观察，我们仍然观察到CAP有时会模糊图像纹理，而DehazeNet会使某些区域变暗。 AOD-Net恢复更丰富和更饱和的颜色（在第三和第四行结果之间进行比较），同时抑制大多数伪像。</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/Fig6.png" alt="" /></p>

<p>c）白色风景自然图像：白色场景或物体一直是去除雾霾的主要障碍。许多有效的先验如[9]在白色物体上失效，因为对于与大气光相似颜色的物体，透射值接近于零。 DehazeNet [3]和MSCNN [27]都依赖于精心挑选的过滤操作进行后处理，这提高了它们对白色物体的稳健性，但不可避免地牺牲了更多的视觉细节。虽然AOD-Net没有明确考虑处理白色场景，但我们的端到端优化方案似乎在这里提供了更强的鲁棒性。图7显示了白色场景的两个模糊图像以及通过各种方法的去雾结果。很容易注意到DCP结果的无法忍受的瑕疵，特别是在第一行的天空区域。这个问题得到缓解，但仍然存在CAP，DehazeNet和MSCNN结果，而AOD-Net几乎没有人工制品。此外，CAP似乎模糊了白色物体的纹理细节，而MSCNN创建了过度增强的相反效果：请参阅猫头部区域进行比较。 AOD-Net设法消除雾霾，而不会引入假色调或扭曲的物体轮廓。</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/Fig7.png" alt="" /></p>

<p>d）对无雾图像的小影响：虽然经过模糊的图像训练，AOD-Net被证实具有非常理想的特性，如果它没有雾度，它对输入图像几乎没有负面影响。这赞同我们的K估计模块的稳健性和有效性。图8显示了来自Colorlines的两个具有挑战性的清晰图像的结果[8]。</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/Fig8.png" alt="" /></p>

<p>e）图像反光晕：我们在另一个图像增强任务上尝试AOD-Net，称为图像反光晕，无需重新训练。光晕是光线超出适当边界的扩散，在照片的明亮区域形成不希望的雾化效果。与去雾相关但遵循不同的物理模型，AOD-Net的反光晕结果也是不错的：参见图9的一些例子。</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/Fig9.png" alt="" /></p>

<h3 id="多尺度特征的有效性">多尺度特征的有效性</h3>

<p>在本节中，我们专门分析了K估计模块的层间级联的有用性，它结合了不同大小的滤波器的多尺度特征。 我们认为，尽管经验性发现，当前的连接方式通过始终将几个连续的较低层连续馈入下一层，促进了从低层到高层的平滑特征转换。 为了进行比较，我们设计了一个基线：“conv1→conv2→conv3→conv4→conv5（K）”，它不涉及层间连接。 对于TestSet A，平均PSNR为17.0517 dB，SSIM为0.7688。 对于TestSet B，平均PSNR为22.3359 dB，SSIM为0.9032。 这些结果通常不如AOD-Net（除了TestSet B上的PSNR略高），特别是两个SSIM值都遭受显着下降。</p>

<h3 id="速度对比">速度对比</h3>

<p>AOD-Net的轻质结构导致更快的除雾。 我们在同一台机器（Intel（R）Core（TM）i7-6700 CPU@3.40GHz和16GB内存）上为所有型号选择了来自TestSet A的50个图像，没有GPU加速。 所有型号的每图像平均运行时间如表IV所示。 尽管其他Matlab实现较慢，但比较DehazeNet（Pycaffe版本）和我们的[11]是公平的。 结果表明AOD-Net具有很高的效率，每张图像仅占DehazeNet的1/10</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/Table4.png" alt="" /></p>

<h2 id="高层视觉任务-评估和提升目标检测任务中的去雾">高层视觉任务：评估和提升目标检测任务中的去雾</h2>

<p>高级计算机视觉任务，如物体检测和识别，涉及视觉语义，并受到了极大的关注[26]，[43]。然而，这些算法的性能可能在很大程度上受到实际应用中各种降级的危害。传统方法在进入目标任务之前采用单独的图像恢复步骤。最近，[40]，[17]验证了恢复和识别步骤的联合优化将显着提高传统两阶段方法的性能。然而，之前的方法[44]，[5]，[4]主要仅针对图像分类任务，研究了常见降级（如噪声，模糊和低分辨率）的影响和补救措施。据我们所知，没有类似的工作来定量研究雾霾的存在如何影响高级视觉任务，以及如何使用联合优化方法来减轻其影响。我们研究了雾霾存在下的物体检测问题，作为高级视觉任务如何与除雾相互作用的一个例子。我们选择faster R-CNN模型[26]作为强基线4，并测试合成和自然有雾图像。然后，我们将AOD-Net模型与更快的R-CNN模型连接起来，作为统一管道进行联合优化。从我们的实验得出的一般结论是：随着雾度变重，物体检测变得不太可靠。在所有雾霾条件下（轻度，中度或重度），我们的联合调谐模型不断改进检测，超越maive fster R-CNN和非联合的方法。</p>

<h3 id="pascal-voc2007上的合成雾霾图像的质量评估">Pascal-voc2007上的合成雾霾图像的质量评估</h3>

<p>A.具有合成雾度的Pascal-VOC 2007的定量结果我们从Pascal VOC 2007数据集（称为Groundtruth）创建了三个合成集[6]：</p>

<p>重雾（A = 1，β= 0.1），
中雾（A = 1，β= 0.06）
轻雾（A） = 1，β= 0.04）。
通过[18]中描述的方法预测深度图。每组被分成非重叠训练集和测试集。首先，我们比较五种方案而不进行任何网络微调：</p>

<p>（1）naive fater - RCNN：使用清晰的Pascal-VOC 2007上预先训练的模型直接输入模糊图像;</p>

<p>（2）DehazeNet +faster R-CNN：首先使用DehazeNet进行除雾，然后使用faster R-CNN;</p>

<p>（3）MSCNN +faster R-CNN：首先使用MSCNN进行除雾，然后使用faster R-CNN;</p>

<p>（4）DCP +faster  R-CNN：首先使用DCP进行除雾，然后使用faster R-CNN;</p>

<p>（5）AOD-Net+faster  R-CNN：AOD-Net与faster  R-CNN连接，没有任何联合调整。</p>

<p>我们计算三个测试集的平均精度（mAP），如表V中的前三行所示。清晰的Pascal-VOC 2007测试集上的mAP为0.6954。我们可以看到，重度雾霾使mAP降低了近0.18。通过在检测之前首先使用各种去雾方法去雾，mAP改善了很多。其中，DCP +faster rcnn4我们使用基于20种Pascal VOC 2007数据集预训练的VGG16模型，由faster  R-CNN作者提供：https：//github.com/ rbgirshick / py-fast-rcnnR-CNN表现最佳，重雾度提高21.57％。没有任何联合调整，AOD-Net +faster R-CNN表现与MSCNN +faster R-CNN相当，并且看起来比DCP +faster R-CNN更差。</p>

<p>由于我们的一体化设计，AOD-Net + Faster R-CNN的流水线可以从头到尾联合优化，以提高对模糊图像的物体检测性能。我们分别为三个雾霾的训练集调整AOD-Net + Faster R-CNN，并将此调谐版称为JAOD-Faster R-CNN。我们在前35,000次迭代中使用0.0001的学习率，在接下来的65,000次迭代中使用0.00001，两者的动量均为0.9，重量衰减为0.0005。由于这种联合调整，重雾度情况下mAP从0.5794增加到0.6819，这显示了这种端到端优化的主要优势和我们独特设计的价值。为了比较，我们还在雾霾数据集上重新训练faster R-CNN作为比较。我们使用0.0001的学习率来微调预训练的更快的R-CNN（在清晰的自然图像上训练）。在重新训练以适应雾霾数据集之后，在重度雾度下，再训练的快速R-CNN的mAP从0.5155增加到0.6756，同时仍然比JAOD-faster R-CNN更差。此外，由于实际上希望获得一个适用于任意雾度水平的统一模型，我们生成包括各种雾度水平的训练集，其中β从[0,0.1]随机采样。我们在这个训练集上重新调整和评估JAOD-Faster R-CNN和Retrained Faster R-CNN，其结果在表V的最后一行进行比较。尽管两者的表现都略逊于他们训练和申请的“专用”对应物。在特定的雾度水平下，它们在所有三种雾度水平下都表现良好，而JAOD-Faster R-CNN再次优于Retrained Faster R-CNN。图11描绘了在各种雾度条件下，在JAOD-Faster R-CNN和再培训更快的R-CNN方案之间每5,000次迭代的mAP比较。</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/Table5.png" alt="" /></p>

<h3 id="自然雾霾图像的质量评估">自然雾霾图像的质量评估</h3>

<p>图10显示了对Web源自然模糊图像的对象检测结果的视觉比较。比较了六种方法： (1) naive Faster-RCNN; (2) DehazeNet + Faster R- CNN ; (3) MSCNN + Faster R-CNN ; (4) AOD-Net + Faster R- CNN; (5) Fine-tuned Faster R-CNN. (6) JAOD-Faster R-CNN.。我们观察到雾霾可能导致错误检测，不准确的本地化以及对于fster R-CNN的不自信的类别识别。虽然AOD-Net + Faster R-CNN已经显示出优于原始Faster-RCNN的明显优势，但JAOD-Faster R-CNN结果的性能进一步显着提升，显着超越所有其他替代方案。</p>

<p>请注意，AOD-Net + Faster R-CNN从联合优化中获益双重：AOD-Net本身共同估算所有参数，整个流水线共同调整低水平（去雾）和高水平（检测）和认可）端到端的任务。端到端的管道调整是由AOD-Net独特实现的，AOD-Net是迄今为止唯一的一体化除雾模型。</p>

<p><img src="http://jchenTech.github.io/images/AOD-Net/Fig11.png" alt="" /></p>

<h3 id="到底那个起到实际帮助-特定任务的除雾网络或只是添加更多参数">到底那个起到实际帮助：特定任务的除雾网络或只是添加更多参数？</h3>

<p>虽然JAOD-Faster R-CNN可以说是上面所示的最佳表现者，但自然会出现一个问题：它是否是AOD-Faster R-CNN使用的参数比（Retrained）faster R-CNN更多的结果？在本节中，我们展示了添加额外的图层和参数，而没有针对去雾的任务特定设计，并不一定能提高雾度中对象检测的性能。我们设计了一个名为Auto-Faster R-CNN的新基线，用简单的卷积自动编码器取代了JAOD-Faster R-CNN中的AOD-Net部分。自动编码器与AOD-Net具有相同数量的参数，由五个卷积层组成，其结构类似于K估计模块。我们使用与AOD-Net相同的训练协议和数据集预先训练用于除雾任务的自动编码器，并将其与更快的R-CNN连接以进行端到端调整。如表VI所示，自动更快的R-CNN的性能与AOD-Faster R-CNN并不相同，并且表现得比微调更快的R-CNN更差。回想一下[26]证实直接向更快的R-CNN添加额外的层并不一定能改善一般干净图像中物体检测的性能。我们的结论是它在朦胧案件中的一贯对应。此外，应该注意的是，尽管JAOD-Faster R-CNN在更快的R-CNN之前添加了AOD-Net，但由于AOD-Net的轻量化设计，复杂性并未增加太多。对于（Retrained）faster R-CNN，每个图像的平均运行时间为0.166秒，对于JAOD-Faster R-CNN，使用NVIDIA GeForce GTX TITAN X GPU为0.192秒。</p>

<h2 id="讨论和结论">讨论和结论</h2>

<p>本文提出了AOD-Net，这是一种一体化的管道，可以通过端到端的CNN直接重建无雾图像。我们使用客观（PSNR，SSIM）和主观标准，在合成和自然雾度图像上比较AOD-Net与各种最先进的方法。广泛的实验结果证实了AOD-Net的优越性，稳健性和效率。此外，我们还提出了第一个关于AOD-Net如何通过联合管道优化提高自然朦胧图像上的物体检测和识别性能的研究。可以观察到，我们共同调整的模型在雾度存在的情况下不断改进检测，超过了天真的更快的R-CNN和非关节方法。然而，如上所述，去雾技术与来自图像的深度估计高度相关，并且通过结合深度先验知识或精细的深度估计模块，存在改善AOD-Net的性能的空间。</p>

<h2 id="参考文章">参考文章</h2>

<p>[1] <a href="https://blog.csdn.net/Julialove102123/article/details/89046288">https://blog.csdn.net/Julialove102123/article/details/89046288</a></p>
    </div>
    <footer class="post-footer">
     
 
<div class="post-tags">     
     
    <a href="http://jchenTech.github.io/tags/%e5%9b%be%e5%83%8f%e5%8e%bb%e9%9b%be" rel="tag" title="图像去雾">#图像去雾#</a>
    
    <a href="http://jchenTech.github.io/tags/aod-net" rel="tag" title="AOD-Net">#AOD-Net#</a>
    
</div>



     <div class="post-nav">
    <div class="post-nav-next post-nav-item">
    
        <a href="http://jchenTech.github.io/post/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0mscnn/" rel="next" title="【论文笔记】MSCNN: Single Image Dehazing via Multi-Scale Convolutional Neural Networks">
        <i class="fa fa-chevron-left"></i> 【论文笔记】MSCNN: Single Image Dehazing via Multi-Scale Convolutional Neural Networks
        </a>
    
    </div>

    <div class="post-nav-prev post-nav-item">
    
        <a href="http://jchenTech.github.io/post/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%9B%86/" rel="prev" title="DehazeZoo：图像&amp;视频去雾清晰化资源大全（论文/数据集/开源项目）">
        DehazeZoo：图像&amp;视频去雾清晰化资源大全（论文/数据集/开源项目） <i class="fa fa-chevron-right"></i>
        </a>
    
    </div>
</div>
      
     
     
     




    </footer>
  </article>
</section>

          </div>
        </div>
        <div class="sidebar-toggle">
  <div class="sidebar-toggle-line-wrap">
    <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
    <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
    <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
  </div>
</div>
<aside id="sidebar" class="sidebar">
  <div class="sidebar-inner">

  <ul class="sidebar-nav motion-element">
    <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
      文章目录
    </li>
    <li class="sidebar-nav-overview" data-target="site-overview">
      站点概览
    </li>
  </ul>

    <section class="site-overview sidebar-panel ">
      <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
        src="http://jchenTech.github.io/img/author.jpg"
        alt="陈建君" />
    <p class="site-author-name" itemprop="name">陈建君</p>
    <p class="site-description motion-element" itemprop="description"> 
        Programmer &amp; Huster</p>
</div>
      <nav class="site-state motion-element">
    <div class="site-state-item site-state-posts">
      <a href="http://jchenTech.github.io/post/">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">日志</span>
      </a>
    </div>
    <div class="site-state-item site-state-categories">    
        <a href="http://jchenTech.github.io/categories/">      
         
        <span class="site-state-item-count">8</span>
        
        <span class="site-state-item-name">分类</span>
        
        </a>
    </div>

    <div class="site-state-item site-state-tags">
        <a href="http://jchenTech.github.io/tags/">
         
        <span class="site-state-item-count">37</span>
        
        <span class="site-state-item-name">标签</span>
        </a>
    </div>
</nav>
      
      
<div class="links-of-author motion-element">
    
        <span class="links-of-author-item">
        <a href="https://github.com/xtfly/" target="_blank" title="GitHub">
            <i class="fa fa-fw fa-github"></i>
            GitHub
        </a>
        </span>
    
        <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/lan-ling-xin-yun" target="_blank" title="知乎">
            <i class="fa fa-fw fa-globe"></i>
            知乎
        </a>
        </span>
    
</div>


      

      <div class="links-of-blogroll motion-element inline">
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5b4f2ucxar6&amp;m=0&amp;s=220&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33&amp;bv=35" async="async"></script>
</div>

    </section>
    
<section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
    <div class="post-toc">
        <div class="post-toc-content"><nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#引言">引言</a>
<ul>
<li><a href="#前人的工作">前人的工作</a></li>
<li><a href="#存在的问题">存在的问题</a></li>
<li><a href="#我们的贡献">我们的贡献</a></li>
</ul></li>
<li><a href="#提出的方法">提出的方法</a>
<ul>
<li><a href="#物理模型">物理模型</a></li>
<li><a href="#网络设计">网络设计</a></li>
</ul></li>
<li><a href="#实验结果">实验结果</a>
<ul>
<li><a href="#数据集和实现">数据集和实现</a></li>
<li><a href="#合成数据集上的质量评估">合成数据集上的质量评估</a></li>
<li><a href="#视觉效果评估">视觉效果评估</a></li>
<li><a href="#多尺度特征的有效性">多尺度特征的有效性</a></li>
<li><a href="#速度对比">速度对比</a></li>
</ul></li>
<li><a href="#高层视觉任务-评估和提升目标检测任务中的去雾">高层视觉任务：评估和提升目标检测任务中的去雾</a>
<ul>
<li><a href="#pascal-voc2007上的合成雾霾图像的质量评估">Pascal-voc2007上的合成雾霾图像的质量评估</a></li>
<li><a href="#自然雾霾图像的质量评估">自然雾霾图像的质量评估</a></li>
<li><a href="#到底那个起到实际帮助-特定任务的除雾网络或只是添加更多参数">到底那个起到实际帮助：特定任务的除雾网络或只是添加更多参数？</a></li>
</ul></li>
<li><a href="#讨论和结论">讨论和结论</a></li>
<li><a href="#参考文章">参考文章</a></li>
</ul></li>
</ul>
</nav></div>
    </div>
</section>

  </div>
</aside>

      </div>
    </main>
   
    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  <span itemprop="copyrightYear">  &copy; 
  2009 - 2020</span>
  <span class="with-love"><i class="fa fa-heart"></i></span>
  <span class="author" itemprop="copyrightHolder">陈建君的技术博客</span>
</div>
<div class="powered-by">
  Powered by - <a class="theme-link" href="http://gohugo.io" target="_blank" title="hugo" >Hugo v0.54.0</a>
</div>
<div class="theme-info">
  Theme by - <a class="theme-link" href="https://github.com/xtfly/hugo-theme-next" target="_blank"> NexT
  </a>
</div>


      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
      <span id="scrollpercent"><span>0</span>%</span>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
<script type="text/javascript" src="http://jchenTech.github.io/js/vendor/jquery/index.js?v=2.1.3"></script>
<script type="text/javascript" src="http://jchenTech.github.io/js/vendor/fastclick/lib/fastclick.min.js?v=1.0.6"></script> 
<script type="text/javascript" src="http://jchenTech.github.io/js/vendor/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
<script type="text/javascript" src="http://jchenTech.github.io/js/vendor/velocity/velocity.min.js?v=1.2.1"></script>
<script type="text/javascript" src="http://jchenTech.github.io/js/vendor/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="http://jchenTech.github.io/js/vendor/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>

<script src="http://jchenTech.github.io/js/vendor/fancybox/jquery.fancybox.pack.js?v=2.1.5"></script>

<script type="text/javascript" src="http://jchenTech.github.io/js/utils.js"></script>
<script type="text/javascript" src="http://jchenTech.github.io/js/motion.js"></script>
<script type="text/javascript" src="http://jchenTech.github.io/js/affix.js"></script>
<script type="text/javascript" src="http://jchenTech.github.io/js/schemes/pisces.js"></script>

<script type="text/javascript" src="http://jchenTech.github.io/js/scrollspy.js"></script>
<script type="text/javascript" src="http://jchenTech.github.io/js/post-details.js"></script>
<script type="text/javascript" src="http://jchenTech.github.io/js/toc.js"></script>

<script type="text/javascript" src="http://jchenTech.github.io/js/bootstrap.js"></script>

<script type="text/javascript" src="http://jchenTech.github.io/js/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true
    },
    "HTML-CSS": { fonts: ["TeX"] }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
</body>
</html>